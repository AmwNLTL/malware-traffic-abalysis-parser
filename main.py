import os
import re
import requests
from bs4 import BeautifulSoup
import zipfile
import io
import json

BASE_URL = "https://www.malware-traffic-analysis.net"
START_YEAR = 2022
END_YEAR = 2024
PASSWORD_PREFIX = "infected_"
OUTPUT_FILE = "parsed_data.json"

def check_url_exists(url):
    try:
        response = requests.head(url)
        return response.status_code == 200
    except Exception as e:
        print(f"Error checking URL {url}: {e}")
        return False

def fetch_html_content(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching URL {url}: {e}")
        return None

def extract_zip_file(zip_content, password):
    extracted_files = []
    try:
        with zipfile.ZipFile(io.BytesIO(zip_content)) as z:
            z.setpassword(password.encode('utf-8'))
            for file_info in z.infolist():
                with z.open(file_info, pwd=password.encode('utf-8')) as f:
                    extracted_files.append({
                        "filename": file_info.filename,
                        "content": f.read().decode('utf-8', errors='ignore')
                    })
        return extracted_files
    except Exception as e:
        print(f"Error extracting ZIP file: {e}")
        return None

def parse_ioc_content(content):
    ioc_data = {
        "sha1": [],
        "sha256": [],
        "other_hashes": [],
        "urls": [],
        "fqdn": [],
        "ips": []
    }
    
    # Extract specific fields
    title_pattern = re.compile(r'^\d{4}-\d{2}-\d{2} \(.*?\) - .*$', re.MULTILINE)
    chain_pattern = re.compile(r'CHAIN OF EVENTS:\n- (.*)', re.MULTILINE)
    sha1_pattern = re.compile(r'\b[A-Fa-f0-9]{40}\b')
    sha256_pattern = re.compile(r'\b[A-Fa-f0-9]{64}\b')
    other_hashes_pattern = re.compile(r'\b[A-Fa-f0-9]{32}\b')
    url_pattern = re.compile(r'(https?://[^\s]+)')
    fqdn_pattern = re.compile(r'https?://([A-Za-z0-9.-]+\.[A-Za-z]{2,})')
    ip_pattern = re.compile(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b')
    
    title_match = title_pattern.search(content)
    chain_match = chain_pattern.search(content)
    sha1_matches = sha1_pattern.findall(content)
    sha256_matches = sha256_pattern.findall(content)
    other_hashes_matches = other_hashes_pattern.findall(content)
    url_matches = url_pattern.findall(content)
    fqdn_matches = fqdn_pattern.findall(content)
    ip_matches = ip_pattern.findall(content)

    if title_match:
        ioc_data["title"] = title_match.group(0)
    if chain_match:
        ioc_data["chain"] = chain_match.group(1)
    if sha1_matches:
        ioc_data["sha1"] = sha1_matches
    if sha256_matches:
        ioc_data["sha256"] = sha256_matches
    if other_hashes_matches:
        ioc_data["other_hashes"] = [h for h in other_hashes_matches if h not in sha1_matches and h not in sha256_matches]
    if url_matches:
        ioc_data["urls"] = url_matches
    if fqdn_matches:
        ioc_data["fqdn"] = list(set(fqdn_matches))  # Remove duplicates by converting to a set and back to list
    if ip_matches:
        ioc_data["ips"] = ip_matches
    
    return ioc_data

def append_to_json_file(data, filename):
    print(f"Appending data to {filename}")  # Debug print
    if not os.path.isfile(filename):
        with open(filename, 'w') as f:
            json.dump([], f)
    with open(filename, 'r+') as f:
        file_data = json.load(f)
        file_data.append(data)
        f.seek(0)
        json.dump(file_data, f, indent=4)
    print(f"Data appended to {filename}")  # Debug print

def main():
    for year in range(START_YEAR, END_YEAR + 1):
        for month in range(1, 13):
            for day in range(1, 32):
                url = f"{BASE_URL}/{year}/{month:02}/{day:02}/index.html"
                print(f"Checking URL: {url}")  # Debug print
                if check_url_exists(url):
                    html_content = fetch_html_content(url)
                    if html_content:
                        soup = BeautifulSoup(html_content, 'html.parser')
                        zip_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.zip')]
                        daily_data = {
                            "title": None,
                            "date": f"{year}-{month:02}-{day:02}",
                            "chain": None,
                            "sha1": [],
                            "sha256": [],
                            "other_hashes": [],
                            "urls": [],
                            "fqdn": [],
                            "ips": []
                        }
                        for link in zip_links:
                            zip_url = f"{BASE_URL}/{year}/{month:02}/{day:02}/{link}"
                            print(f"\tDownloading ZIP: {zip_url}")  # Debug print
                            try:
                                zip_response = requests.get(zip_url)
                                zip_response.raise_for_status()
                                password = f"{PASSWORD_PREFIX}{year}{month:02}{day:02}"
                                extracted_files = extract_zip_file(zip_response.content, password)
                                if extracted_files:
                                    for file in extracted_files:
                                        if 'IOCs' in file['filename']:
                                            print(f"\t\tParsing file: {file['filename']}")  # Debug print
                                            data = parse_ioc_content(file['content'])
                                            if not daily_data["title"]:
                                                daily_data["title"] = data.get("title", None)
                                            if not daily_data["chain"]:
                                                daily_data["chain"] = data.get("chain", None)
                                            daily_data["sha1"].extend(data.get("sha1", []))
                                            daily_data["sha256"].extend(data.get("sha256", []))
                                            daily_data["other_hashes"].extend(data.get("other_hashes", []))
                                            daily_data["urls"].extend(data.get("urls", []))
                                            daily_data["fqdn"].extend(data.get("fqdn", []))
                                            daily_data["ips"].extend(data.get("ips", []))
                            except Exception as e:
                                print(f"Error downloading or extracting ZIP file {zip_url}: {e}")
                        # Remove duplicates
                        daily_data["sha1"] = list(set(daily_data["sha1"]))
                        daily_data["sha256"] = list(set(daily_data["sha256"]))
                        daily_data["other_hashes"] = list(set(daily_data["other_hashes"]))
                        daily_data["urls"] = list(set(daily_data["urls"]))
                        daily_data["fqdn"] = list(set(daily_data["fqdn"]))
                        daily_data["ips"] = list(set(daily_data["ips"]))
                        print(daily_data)
                        if daily_data["title"]:  # Ensure we have a title before appending
                            append_to_json_file(daily_data, OUTPUT_FILE)
                            daily_data = []
                        else:
                            print("No data collected.")  # Debug print

if __name__ == "__main__":
    main()
